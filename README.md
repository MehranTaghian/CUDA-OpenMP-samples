# Parallel Programming on CPU and GPU

This repository contains some sample code that are useful in learning how to program with OpenMP and CUDA. Here I'm
going to go through each directory and explain the purpose of each file.

## OpenMP

In the OpenMP section, there is a sample code in `parallel_for_loop.cpp` which, as the name suggests, it's a simple
for-loop parallelization.

In the `matrix_add.cpp` code, we have three 2D matrices A, B, and C where we want to calculate C = A + B. We do this in
two ways: i) row-wise parallelization using a single parallel for-loop or ii) parallelize nested for-loops using the
`collapse()` argument. The elements of A and B matrices are initialized randomly between 0 and 100. The
`schedule({static, dynamic}, chunk_size)`
specifies how the thread allocation should be. If the schedule is `static`, it statically allocates a `chunk_size`
number of
successive iterations to a single thread. In the case of `dynamic` allocation, each thread dynamically selects a number
of chunks
of iterations and runs them.

In the `Mandelbrot` section, we have the parallelization of
the [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set) generation, which generates nice images.
The `serial.cpp` file contains the serial code for generating this set. Here we are trying to parallelize this code
using OpenMP. The first step is to parallelize the main **for-loop** in the `For.cpp`. The second technique is
**Multi-Tasking** which generates multiple tasks to do the process.
One major problem that reduces the speed of the process is writing to file. We can
use the `Pipeline` technique (as we did in `Pipeline.cpp`) to parallelize both: i) set generation and ii) writing to
file.

In the `PrefixSum` section, we are parallelizing
the [Prefix sum](https://en.wikipedia.org/wiki/Prefix_sum#:~:text=In%20computer%20science%2C%20the%20prefix,1) problem.
The `serial.cpp` contains the serial code for ths problem. The naive parallelization of the program creates the
challenge of
race condition that can reduce the efficiency of parallelism. You can see first parallelization in `parallel_simple.cpp`
. Another
approach in parallelizing this problem is
the [Hillis and Steele algorithm](https://www.geeksforgeeks.org/hillis-steele-scan-parallel-prefix-scan-algorithm/).
Although this algorithm might be slover than the serial mode, but it is faster when programmed and run on GPUs.

## CUDA

We start the CUDA section with a test program generated by the Visual Studio for making sure that CUDA works. This
program in under `VectorAdd` directory where we brought the serial code in `serial.cpp`, the parallelized code using
OpenMP
in `parallel_omp.cpp`, and finally the parallel code on GPU in `parallel_cuda.cu`.

In the OpenMP section, we had a `matrix_add.cpp` program in which the elements of two matrices A and B are summed and
stored
in a result matrix C. We have the same program in the CUDA section `matrix_add.cu`, where we do the same process, except
that the matrices are
huge and the parallelization is being done on GPU.

Under the `MatrixMult` section, we parallelized the matrix multiplication operation. The serial program is
in `serial.cpp`.
First, we have a naive parallelization in `single_block.cu` where we use only a single block of threads in GPU to
multiply
two matrices. The number of threads in a single block is limited to 1024. Therefore, for matrices with size larger than
1024,
we should either use **tiling** method or use multiple blocks. It is not efficient to use a single block of threads
while we have
multiple free blocks. Therefore, `multiple_blocks.cu` program does this operation using multiple blocks (without tiling)
. We combined the tiling method with multiple blocks in `multiple_blocks_tiling.cu`.
Also an example of the matrix multiplication operation using cuBLAS library is given in `matmul_cuBLAS.cu`.

Another section dedicates to **streaming** multiple kernels, and the concept of **pageable** and **pinned memory**. For
information
about pageable and pinned memory, please visit the following
useful [link](https://leimao.github.io/blog/Page-Locked-Host-Memory-Data-Transfer).
Also for streaming, visit [link](https://leimao.github.io/blog/CUDA-Stream/).
We use the `Histogram` problem as the example for this section. In the Histogram problem, the goal is to count the
number of
different elements in a large array. In the `pageable.cu` we used the pageable memory allocated using the `malloc()`
function. In the `pinned.cu`, the memory is allocated using `cudaMallocHost()` and is page-locked which makes it faster
to access the host memory. In order to use streaming and concurrent kernels, the memory requires to be pinned. In the `streaming.cu`
we ran multiple kernels in parallel using page-locked memory.